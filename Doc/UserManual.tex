\documentclass{article}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}
\usepackage{hyperref}
\begin{document}
\title{ALPIDE Software - User manual}
\date{rev. 3, \today}
\author{Markus Keil, Felix Reidt}
\maketitle

This manual describes the software used to perform standard tests of the ALPIDE chip with different readout boards and in different setups (single chip with DAQ board, HICs or staves with MOSAIC board...).
The manual is intended as a quick start guide to get the ALPIDE
software installed and perform the most important tests via the
command line interface. For a description on the software structure and how to implement your own test applications, please refer to the developer's manual .


\section {Installation and First Steps}
\subsection{Prerequisites}
The software is currently supported to run only on CentOS CERN 7. This reference installations are used to test software after every commit to the repository.
\begin{itemize}
\item \textbf{CentOS CERN 7:}
  \begin{lstlisting}
    yum install -y gcc gcc-c++ make cmake cmake3 tar zlib wget subversion clang \
      git libusb1-devel tinyxml-devel qt5-qtbase-devel krb5-workstation cern-get-sso-cookie
    yum install -y centos-release-scl
    yum update -y
    yum install -y llvm-toolset-7 devtoolset-7
    wget -O /opt/root.tar.gz \
      https://root.cern.ch/download/root_v6.14.00.Linux-centos7-x86_64-gcc4.8.tar.gz
    cd /opt/ ; tar xzfv root.tar.bz
  \end{lstlisting}
  ROOT has to be loaded using
  \begin{lstlisting}
    source /opt/root/bin/thisroot.sh .
  \end{lstlisting}
  Do not install ROOT using \texttt{yum} as the packages are broken!

  Activate the \texttt{llvm-toolset-7} and \texttt{devtoolset-7}:
  \begin{lstlisting}
    source scl_source enable llvm-toolset-7
    source scl_source enable devtoolset-7
  \end{lstlisting}
\end{itemize}

For other operation systems, please see \url{https://gitlab.cern.ch/alice-its-alpide-software/alice-its-docker-containers}. Please note that we cannot support other operating systems that CentOS CERN 7.

\subsection{Installation}
The software is available in a gitlab repository:

\begin{lstlisting}
https://gitlab.cern.ch/alice-its-alpide-software/new-alpide-software
\end{lstlisting}

To check it out for the first time use

\begin{lstlisting}
git clone https://username@gitlab.cern.ch/alice-its-alpide-software/new-alpide-software.git
\end{lstlisting}

with your user name. Note that your account needs to be added to a
list of users in order to being able to access the repository.
Versions are updated regularly, make sure you have checked out the
latest version. After the clone you will find the software in a directory \texttt{new-alpide-software}, an additional subdirectory \texttt{analysis} contains standard macros to analyse the data.

Preferably, the software is compiled using a cmake-based build
flow. For details see the README.md in the software repository.

LEGACY build: The standard software is compiled using
\begin{lstlisting}
make
\end{lstlisting}
The libraries which are necessary for the GUI or EUDAQ are compiled using
\begin{lstlisting}
make lib && make lib_analysis
\end{lstlisting}
The GUI can be compiled using
\begin{lstlisting}
cd GUI
qmake -makefile #qmake-qt5 on SLC6 and CC7
make
\end{lstlisting}
and can be run by invoking
\begin{lstlisting}
source ./env.sh
./GUI
\end{lstlisting}
In order to establish connection with the database Kerberos needs to be configured. In CC7 the configuration can be done as below
\begin{lstlisting}
sudo cp new-alpide-alpide-software/Doc/etc/krb5.conf /etc/krb5.conf
\end{lstlisting}
After the configuration the user needs to create a ticket. This is done by using
the credentials as following
\begin{lstlisting}
kinit <service account>
\end{lstlisting}
The user can check whether a ticket was created or not and until when it is valid, with the command
\begin{lstlisting}
klist
\end{lstlisting}
Once the ticket expires (after 3 days e.g.) the user needs to create a new one as before by using knit

\paragraph{Executing Tests}\hfill \\
After compiling the software package you will find a certain number of standard tests, described below, in the form of individual executables. Each executable performs a specific test once and, if applicable, writes the output data in a text file in the subdirectory \texttt{Data}. The settings of the chip(s), readout board(s) and the scan itself can be modified in the config file \texttt{Config.cfg}. Some of the available parameters are described below, for a more detailed description of the syntax and the available parameters please refer to the comments in the config file itself.

\paragraph{Executing Tests with the DAQ Board}\hfill \\
To execute tests with the DAQ board setup the FX3 chip has to be configured. The procedure and the necessary files are identical to the old pALPIDEfs-software:
\begin{verbatim}
./download_fx3 -t RAM -i SlaveFifoSync.img
\end{verbatim}

In case you have not performed tests with the DAQ board before you can download the necessary files from the directory \texttt{fx3} in the repository https:// gitlab.cern.ch/alice-its-alpide-software/pALPIDEfs-software. You can then compile the tool to configure the FX3 chip (execute \texttt{build\_mac.sh} or \texttt{build\_linux.sh}, resp., in the directory \texttt{fx3}).


\section{Setup Definition}

The standard tests of the ALPIDE software, provided in command line applications, work in the same way for different types of setups. There are several predefined setups that can be selected via the config file. Each of these predefined setup types implements a readout board (DAQ or MOSAIC), a certain number of chips with predefined chip IDs and mapping of chips to the data lines and command interfaces of the MOSAIC board.

Several config-file switches exist to modify these pre-defined setups, e.g. to assign different chip IDs, command interfaces or data receivers to individual chips.

The currently available setups are the following:

\begin {enumerate}
\item Single chip: a setup with a single chip connected to the DAQ board is selected by the config-file switch:
\begin{verbatim}
DEVICE CHIP
\end{verbatim}

In this case the chip has the chip-ID 16 (OB Master), which is the standard configuration for single chip tests. If needed this can be changed by adding a switch \texttt{CHIPID\_0 x} (Since there is only one chip in the setup the index \texttt{\_0} can also be omitted.)

\item Single chip with MOSAIC: to test a single chip with the MOSAIC board the switch
\begin{verbatim}
DEVICE CHIPMOSAIC
\end{verbatim}
has to be used. In this case the software assumes a chip mounted on a carrier with SAMTEC connector, which is connected through an IB Firefly-Eyespeed adapter to the MOSAIC. The default chip ID is 16. In case a different setup is used, mappings of control and data lines need to be changed using the switches \texttt{CONTROLINTERFACE\_0 x} and \texttt{RECEIVER\_0 y}, substituting the appropriate values for x and y.

\item Inner Barrel HIC: test of an inner barrel HIC is done using the switch
\begin{verbatim}
DEVICE IBHIC
\end{verbatim}
This assumes an inner barrel HIC with 9 chips with chip IDs ranging from 0 to 8. The HIC is supposed to be connected to the MOSAIC via a standard IB Firefly-Eyespeed adapter. Again all basic settings (mapping and chip IDs) can be changed.

\item Outer Barrel HIC: test of an outer barrel HIC is done using the switch
\begin{verbatim}
DEVICE OBHIC
\end{verbatim}
This assumes an outer barrel HIC with 14 chips with chip IDs ranging from 16 to 22 and from 24 to 30. The The HIC is supposed to be connected to the MOSAIC via a standard OB Firefly-Eyespeed adapter. Again all basic settings (mapping and chip IDs) can be changed. When changing individual chip settings, note that the index of the config-file switch refers to the running index and not to the chip ID. These are identical for the IB, but not for the OB HIC. The correspondence between chip index and (default) chip ID is shown in the following table:
\begin{center}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|}\hline
Index & 0 & 1 & 2 & ... & 6 & 7 & 8 & ... & 13\\ \hline
Chip ID & 16 & 17 & 18 & ... & 22 & 24 & 25 & ... & 30\\ \hline
\end{tabular}
\end{center}
This numbering scheme corresponds to a module ID of 1. In a future implementation it will be possible to change this via config file, currently this can already be done by changing the individual chip IDs.

\item Outer Barrel Half-Stave: The structure for an outer barrel half-stave is prepared with the device \texttt{HALFSTAVE}, however this device type requires in addition the number of modules, e.g. for a fully mounter outer layer half-stave:
\begin{verbatim}
DEVICE HALFSTAVE
NMODULES 7
\end{verbatim}
The prepared setup then contains \texttt{NMODULES} modules with 14 chips each and ascending module IDs 0 ... \texttt{NMODULES + 1}. If the module IDs are to be changed, this currently has to be done chipwise, however modulewise switches are under preparation.

\end{enumerate}



\paragraph {Chip Enabling / Disabling} \hfill \\
By default all defined chips are enabled for configuration and scans. For all multi-chip devices single chips can be disabled in the config file by adding a line \texttt{ENABLED\_n 0} to disable chip \texttt{n}. In addition the software checks before execution of the actual test whether an enabled chip responds to the command interface. If this is not the case the chip is auto-disabled. In case of the OB module the daisy chain is established after this procedure, skipping all disabled chips \footnote{Note that it is currently not possible to read out a daisy chain with the corresponding master disabled}.


\section{Tests}
This section describes the standalone test programs provided with the ALPIDE software. Each test is executed by running an individual command line executable. Configurations of test parameters (like number of mask stages, number of injections etc.) can be set in the config file \texttt{Config.cfg}. If the test generates output data this is written to the subdirectory \texttt{Data}

\subsection{FIFO Test}
The FIFO test is a quick test to check the communication with the
chips' control interfaces. It writes three different bit patterns (\texttt{0x0000, 0xffff} and
\texttt{0x5555}) into each cell of the
end-of-column FIFOs, reads them back and checks the correctness of the
readback value. The test is executed by running
\begin{verbatim}
./test_fifo
\end{verbatim}

The test will print on screen the numbers of errors for each chip (seperately for each pattern) and at the end the sum of errors for all tested chips.
(If for debugging purposes output of the individual errors is needed the program has to be recompiled setting the variable \texttt{bool Verbose} in \texttt{main\_fifo.cpp} to \texttt{true}).

\subsection {On-chip DAC Scan (DAQ board only)}
The output of the on-chip DACs can be connected to monitoring pins of
the ALPIDE chip and measured by ADCs on the DAQ board. The DAC Scan measures the characteristics of all on-chip DACs. The test is started by running
\begin{verbatim}
./test_dacscan
\end{verbatim}
For each DAC it loops over the values from 0 to 255 and measures the
output values. The measured values are written into one file for each
DAC.

\subsection{Digital Scan}
The digital scan generates a digital pulse in a number of pixels and
reads the hits out. It is started with the command
\begin{verbatim}
./test_digital
\end{verbatim}

The number of injections and the number of injected pixels is configurable with the parameters \texttt{NINJ, PIXPERREGION, NMASKSTAGES} in the config file (see also the comments in the box below).

The output data is written into a file \texttt{DigitalScan.dat}, each
line has the format
\begin{verbatim}
Doublecol Address NHits
\end{verbatim}

with \texttt{Doublecol} ranging from 0 to 511, \texttt{Address} from 0
to 1023 (Address is the address as described in the ALPIDE manual,
not the row number).

\paragraph {Analysis Macro} \hfill \\
In the subdirectory \texttt{analysis} you can find the macro \texttt{Hitmap.C}, which can be used to visualize the output data of the digital scan. If the macro is invoked in the form
\begin{verbatim}
.x Hitmap.C (<filename>, <number of injections>)
\end{verbatim}
it draws a hitmap of the given chip and returns the numbers of pixels that have registered 0 hits or more / less hits than injections. If the second parameter is omitted, only the hitmap is drawn.

\vspace{0.5 cm}
\fbox{\parbox{\textwidth}{General remarks on scans:

\begin{itemize}
\item{Mask stages: All injection-based scans (digital, analogue and
    threshold) work on a certain number of pixels at a time. This number of pixels is configurable by the parameter \texttt{PIXPERREGION} in the config file, which gives the number of pixels per region (!) that is injected simultaneously. (A value of 1 corresponds to 32 pixels per chip. The maximum value is 32, which corresponds to a complete row.) After the required number of injections  has been done the
  scan moves to the next set of pixels. The number of such mask stages that is performed is configurable with the parameter \texttt{NMASKSTAGES}. In order to scan the entire chip the mask has to be staged 16384 / \texttt{PIXPERREGION} times, a lower number accordingly leads to a percentage $<100\%$ of scanned pixels.}
\item{Output files: due to the large amount of data in particular for
    threshold scans, output data is written only for pixels with $>0$ hits. In case of multichip devices, one output file per chip is created.}
\end{itemize}}}


\subsection{Threshold Scan}
The threshold scan performs analogue injections, looping over the
charge. For each charge point 50 injections are performed. The command
is
\begin{verbatim}
./test_threshold
\end{verbatim}
The output file \texttt{ThresholdScan.dat} contains the raw data, i.e. the
number of hits for each charge point, in the format
\begin{verbatim}
Doublecol Address Charge NHits
\end{verbatim}
In addition to the parameters present for the digital scan, for the threshold scan also the charge range can be configured in the config file; the default setting ranges from 0 to 50 DAC units.

\paragraph {Analysis Macro} \hfill \\
In the subdirectory \texttt{analysis} you can find the macro \texttt{FitThresholds.C}, which is used to fit the s-curves for the scanned pixels. For speed reasons it should be used in the compiled form
\begin{verbatim}
.x FitThresholds.C+ (<filename>)
\end{verbatim}

The fitted values are written in an output file \texttt{FitValues... .dat} in the format \texttt{doublecol address thresh noise chisq}, which can be visualized in form of a threshold map with the macro \texttt{ThresholdMap.C} in the same directory.


\subsection{Noise Occupancy}
The scan gives a selectable number of random triggers and returns the
number of hits. The command is
\begin{verbatim}
./test_noiseocc
\end{verbatim}
The output file format is identical to the digital scan.

\paragraph {Analysis Macro} \hfill \\
The macro \texttt{NoiseOccupancyRawToHisto} in the subdirectory \texttt{analysis} can be used to prepare a histogram with the noise occupancy as a function of the number of masked pixels. The path to the file, which is to be analysed, has to be given as a parameter. Note that the macro assumes a certain number of triggers, which is read from a config file in case of the DAQ board. If the MOSAIC is used this config file does not exist and the macro has to be adjusted accordingly (i.e. remove the reading of the config file and set the variable \texttt{n\_trg} manually to the correct number of triggers). This will be adjusted in a future version.

\subsection{Source Scan}
A dedicated source scan is currently not implemented, however by adjusting the strobe length the noise occupancy scan can be used to perform a simple source scan (without single event information). The hitmap macro can then be used to show the hit data.



\end{document}
